{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "blind-pride",
   "metadata": {},
   "source": [
    "<!-- dom:TITLE: Homework 1, PHY 959 -->\n",
    "<!-- dom:AUTHOR: [PHY 959: Machine Learning in Physics]-->\n",
    "\n",
    "\n",
    "# PHY 959: Homework Set #1\n",
    "Due: **January 28, 2021**\n",
    "\n",
    "\n",
    "\n",
    "___\n",
    "***\n",
    "\n",
    "\n",
    "# Problem 2: Maximum Likelihood and Linear Regression\n",
    "\n",
    "In this problem we will explore linear regression using both our your own code as well as with modules imported from scikit-learn.  However, we'd also like to remind ourselves how similar linear regression is to maximum likelihood, so we will start by reminding ourselves about how that works!  We will use the Iris Flower dataset from Homework 1 Problem 1 to \"seed\" our fitting challenge with data.\n",
    "\n",
    "As you should have been able to observe in Problem 1, there is a high degree of correlation between petal length and petal width.  We are going to perform a two parameter fit to this data of the form:\n",
    "\n",
    "$f(x) = \\theta_0 + \\theta_1 \\times x$\n",
    "\n",
    "This linear relationship should match well with the correlation between petal length and width.  This problem will have three parts:\n",
    "\n",
    "  * Find the maximum likelihood estimators for $\\theta_0$ and $\\theta_1$ for the Iris data\n",
    "  * Write your own code to perform linear regression to find $\\hat{f}(x)$ for the Iris data\n",
    "  * Use imported modules from scikit-learn to perform linear regression to find $\\hat{f}(x)$ for the Iris data\n",
    "\n",
    "***\n",
    "\n",
    "## Part 1:\n",
    "\n",
    "The first thing we need to do is to remind ourselves how maximum likelihood works.  Use a Gaussian likelihood model to evaluate the ML estimators (MLEs) for the slope and intercept for a linear fit to the Iris Flowers dataset features petal width (abscissa or \"x-axis\") and petal length (ordinate or \"y-axis\").  Feel free to perform the maximum likelihood calculation numerically (stepping through values for each variable in a grid) or analytically (taking derivatives of the likelihood function).  If it helps you to visualize things, make a plot of the 2D likelihood function.  I've provided some starter code for splitting the data set into arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "# Load dataset\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv\"\n",
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n",
    "dataset = pd.read_csv(url, names=names)\n",
    "\n",
    "#break the array into individual arrays\n",
    "array = dataset.values\n",
    "\n",
    "#petal width\n",
    "X1 = array[:,3]\n",
    "\n",
    "#petal length\n",
    "X2 = array[:,2]\n",
    "\n",
    "#labels\n",
    "y = array[:,4]\n",
    "\n",
    "print(X1)\n",
    "\n",
    "pyplot.scatter(X1,X2)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-employer",
   "metadata": {},
   "source": [
    "## Part 2:\n",
    "\n",
    "Now write code to perform a linear regression of the Iris Flowers petal width and length.  To do so, build a design matrix that includes a bias term (ie, the offset or $\\theta_0$).  If you use mean squared error, you can turn this into a relatively straight-forward linear algebra problem.  I strongly encourage you to use the NumPy `linalg` module to invert matrices (`np.linalg.inv()`) and to perform dot products (`np.linalg.dot()`).  You should be able to do all of this in less than 10 lines of code, so if you're finding yourself writing a novel please check with Prof. Fisher.\n",
    "\n",
    "### Part 2a:\n",
    "Print out your fitted parameters.  Be sure to indicate which variable is the bias and which is the slope parameter.\n",
    "\n",
    "### Part 2b:\n",
    "Make a plot of the data along with your linear regression fit.  Does it make sense to you?\n",
    "\n",
    "### Part 2c:\n",
    "Calculate the loss function for this data set. We're assuming mean squared error, so that should be a one-liner.  But if you're going to use that calculation a lot, you may want to define a function.\n",
    "\n",
    "### Part 2d:\n",
    "Reflection.  Was that easier or harder to evaluate compared to the MLE approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-advocate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "czech-linux",
   "metadata": {},
   "source": [
    "## Part 3:\n",
    "\n",
    "Last part!  Now let's reach out to scikit-learn to see if we can use their tools to get the (hopefully!) same answer for the case of mean-squared error.  \n",
    "\n",
    "### Part 3a: \n",
    "Did you get the same coefficients in your scikit-learn linear regression?  \n",
    "What about the mean squared error?\n",
    "Hint: you should have!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-cement",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

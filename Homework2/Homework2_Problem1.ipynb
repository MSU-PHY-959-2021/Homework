{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-valve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bit of setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Create color maps\n",
    "cmap_list = ['orange', 'cyan', 'cornflowerblue']\n",
    "cmap_bold = ['darkorange', 'c', 'darkblue']\n",
    "cmap_light = ListedColormap(cmap_list)\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-sheet",
   "metadata": {},
   "source": [
    "<!-- dom:TITLE: Homework 1, PHY 959 -->\n",
    "<!-- dom:AUTHOR: [PHY 959: Machine Learning in Physics]-->\n",
    "\n",
    "\n",
    "# PHY 959: Homework Set #2\n",
    "Due: **February 9, 2021**\n",
    "\n",
    "\n",
    "\n",
    "___\n",
    "***\n",
    "\n",
    "\n",
    "# Problem 1: Exploring Regularization\n",
    "\n",
    "In this problem, we will exercise what we learned about regularization.  We will use two different data sets that allow us to see regularization at work with a linear regression problem.  You will be fitting a polynomial function to data that is mostly linear, but has enough noise to confuse our choice of polynomial function.  Because we don't technically know the highest order of the polynomial in the data, we need to allow for a wide range.  But if we use a high enough order, we'll easily overfit our data and lose the ability to predict new data.  The goal will be to learn to identify cases where we've overfit our data and to use regularization to reduce the overfitting.\n",
    "\n",
    "There is a lot of \"starter\" code here that will not work out of the box.  You'll need to add your own code to fill in some gaps that are missing.  Ask Prof. Fisher if you get stuck.\n",
    "\n",
    "## Part 1a:\n",
    "\n",
    "In the next cell, you are given:\n",
    "\n",
    "  1. A training data sample to use to fit your polynomial\n",
    "  2. An independent testing data sample to evaluate the quality of your fit\n",
    "\n",
    "Make a plot of these two samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-delicious",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and visualize the data\n",
    "\n",
    "# Make some training data\n",
    "nPts = 10\n",
    "np.random.seed(119)\n",
    "Xtrain = np.random.uniform(0,1,nPts)\n",
    "ytrain = 0.1*Xtrain + 1*np.sin(Xtrain*15) \n",
    "\n",
    "# Make some testing data, 2x as many as training\n",
    "np.random.seed(121)\n",
    "Xtest = np.random.uniform(0,1,nPts*2)\n",
    "ytest = 0.1*Xtest + 1*np.sin(Xtest*15)\n",
    "\n",
    "# Because we're going to be plotting lines, it will be very convenient if\n",
    "# our data is ordered by the absyssa.  We can use the argsort function to create a permutation.\n",
    "print(\"Before permutation\\n\",Xtrain)\n",
    "permute = Xtrain.argsort()\n",
    "Xtrain=Xtrain[permute]\n",
    "ytrain=ytrain[permute]\n",
    "print(\"After permutation\\n\",Xtrain)\n",
    "\n",
    "permute = Xtest.argsort()\n",
    "Xtest=Xtest[permute]\n",
    "ytest=ytest[permute]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-enlargement",
   "metadata": {},
   "source": [
    "## Part 1b:\n",
    "\n",
    "Create a design/feature matrix for your samples.  In previous examples, the design matrix contained one variable for each dimension. Because we're now doing polynomial regression in one dimension, you will need to create a d-dimensional design matrix where d is the highest polynomial order.  For this problem, use d=7.  You can try out other values, but when you turn in your work please use 7 degrees of freedom.  Note that one of these will be the offset (bias) variable.\n",
    "\n",
    "I strongly encourage you to try out the module ```PolynomialFeatures``` from ```scikit-learn```:\n",
    "\n",
    "```\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "xt = np.zeros((X.size,1))\n",
    "xt[:,0] = Xtrain\n",
    "poly = PolynomialFeatures(degree=nDegr)\n",
    "DM = poly.fit_transform(xt)  #this is your design matrix!\n",
    "```\n",
    "\n",
    "In this example code, note that the variable to be transformed needs to be an array of size ```(Npts,1)```.  Print out your design matrix so you can see what the ```PolynomialFeatures``` module does for you.  Don't forget to build a design matrix for your testing data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "nDegr = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-watershed",
   "metadata": {},
   "source": [
    "## Part 1c:\n",
    "\n",
    "Using your skills learned from homework #1, use the Numpy ```linalg``` module to calculate the vector of weights that best fit your data.  Make a plot of your training data and your best fit line.  A convenient way to generate your line would be like this:  ```line = np.dot(DM,betas)``` wherein ```betas``` is the weights vector from your ```linalg``` solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-business",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate our weights\n",
    "betas = \n",
    "\n",
    "#print out our betas\n",
    "print(betas)\n",
    "\n",
    "#make a line!\n",
    "line = np.dot(DM,betas)\n",
    "\n",
    "#plot the line!\n",
    "plt.scatter(Xtrain,ytrain)\n",
    "plt.plot(Xtrain,line,c=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-facility",
   "metadata": {},
   "source": [
    "## Part 1d:\n",
    "\n",
    "Now let's set up to do a fit of our data and add regularization to the mix.  I've provided a starting point to speed you up.  Note that there is a \"toggle\" to switch between L2 and L1 regularization.  You'll need to work out the correct gradients for each to make the code work properly.\n",
    "\n",
    "In the following, you'll be making some plots. You do not need to turn in plots using both regularization schemes.  It is enough that the plots can be generated as you toggle between L2 and L1 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-valentine",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear regression\n",
    "\n",
    "# Initialize parameters from our fit.  To reduce training time, we may as well\n",
    "# use the results from our previous step! \n",
    "W = betas*0.99\n",
    "\n",
    "# some hyperparameters\n",
    "step_size = 0.1  # Note that if the learning rate is too big, you may not converge!\n",
    "reg = 0.5 # regularization strength\n",
    "\n",
    "#Toggle between L2 and L1 regularization\n",
    "doL2 = 0\n",
    "\n",
    "Nsteps = 10  # number of steps for the regularization strength to be tested over\n",
    "rMin = 0.0001\n",
    "rMax = 5\n",
    "\n",
    "# toggle L1 and L2 regularization\n",
    "if doL2:\n",
    "    rMax = 1\n",
    "\n",
    "# this array will hold the values of the regularization strength that we'll test\n",
    "regRange = np.arange(rMin,rMax,(rMax-rMin)/Nsteps)\n",
    "\n",
    "# this array will hold the results of our various fits\n",
    "weights = np.zeros((regRange.size,nDegr+1))\n",
    "\n",
    "\n",
    "# gradient descent loop\n",
    "Niter = 250\n",
    "for ridx, reg in enumerate(regRange):\n",
    "    #always reset the weights between regularization cycles!\n",
    "    W = betas*0.999\n",
    "    \n",
    "    for i in range(Niter):\n",
    "  \n",
    "        # evaluate function values\n",
    "        fhat = np.dot(DM,W)\n",
    "\n",
    "        # compute mean squared loss\n",
    "        data_loss = np.sum((ytrain-fhat)**2)\n",
    "        data_loss /= nPts\n",
    "\n",
    "        # Regularize!\n",
    "        # L2 regularization\n",
    "        if doL2 :\n",
    "            reg_loss = \n",
    "        else:\n",
    "            reg_loss = \n",
    "\n",
    "        # Total loss is the sum\n",
    "        loss = reg_loss + data_loss\n",
    "\n",
    "        # Report progress\n",
    "        if i % 100 == 0:\n",
    "            print(\"Reg %.3f, iteration %d: loss %f\" % (reg,i, loss))\n",
    "        \n",
    "        #compute the loss gradients\n",
    "        dW = -2*np.dot(DM.T,ytrain-fhat)\n",
    "        dW /= nPts\n",
    "        \n",
    "        if doL2:\n",
    "            dW +=  # regularization gradient for L2\n",
    "        else:            \n",
    "            dW +=  # regularization gradient for L1\n",
    "        \n",
    "        #update weights after this iteration\n",
    "        W -= step_size*dW\n",
    "\n",
    "    #capture weights\n",
    "    weights[ridx,:] = W\n",
    "    print(\"Added: \", ridx)\n",
    "    \n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-exhaust",
   "metadata": {},
   "source": [
    "## Part 1e:\n",
    "\n",
    "You now have a weights matrix that is of size ```(regRange.size,nDegr+1)```, wherein each row is the weights found for a given regularization strength.  Make two figures using this data:\n",
    "\n",
    "  1. Plot the value of each of your ```nDegr``` parameters (ordinate) as a function of the regularization strength (absyssa)\n",
    "  2. Plot the resulting function found for each regularization strength value.  Include the training data in the figure.\n",
    "  \n",
    "Please use your figures to support answers to the following questions:\n",
    "\n",
    "  1. What do you conclude about your choice of regularization scheme, L2 vs L1?\n",
    "  2. At this point, can you state which regularization strength is \"best\" for each case?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-template",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-illinois",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "million-helena",
   "metadata": {},
   "source": [
    "## Part 1f:\n",
    "\n",
    "Last one!!  To better understand the effect of regularization on this problem, let's study the testing data sample.  Make a final plot that shows the mean squared error loss as a function of regularization strength.  Note that you should be able to reuse a lot of the code from the previous step.  Be careful about finding the mean value, as your testing and training data samples are not the same size!\n",
    "\n",
    "Use this figure and the previous two to answer the following:\n",
    "\n",
    "  1. Which regularization strength appears to be best for both L1 and L2 regularization?\n",
    "  2. For this problem, which regularization scheme appears to work better?  Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-latter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

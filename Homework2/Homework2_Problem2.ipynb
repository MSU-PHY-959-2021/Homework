{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-remedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bit of setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Create color maps\n",
    "cmap_list = ['orange', 'cyan', 'cornflowerblue']\n",
    "cmap_bold = ['darkorange', 'c', 'darkblue']\n",
    "cmap_light = ListedColormap(cmap_list)\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-topic",
   "metadata": {},
   "source": [
    "<!-- dom:TITLE: Homework 1, PHY 959 -->\n",
    "<!-- dom:AUTHOR: [PHY 959: Machine Learning in Physics]-->\n",
    "\n",
    "\n",
    "# PHY 959: Homework Set #2\n",
    "Due: **February 9, 2021**\n",
    "\n",
    "\n",
    "\n",
    "___\n",
    "***\n",
    "\n",
    "\n",
    "# Problem 2: Exploring Ensembles\n",
    "\n",
    "In this problem, we will exercise what we learned about using ensembles to improve the reliability of our training.  We will continue to use sparse datasets with one dimension, as it's easier to visualize.  However, these methods will be useful in the rest of the course material...especially with neural networks and boosted decision trees.  \n",
    "\n",
    "Parts 2a-2c will be almost identical to what you already did for Problem 1, so use what you did there and copy/paste.  That will not be considered plagiarism.\n",
    "\n",
    "There is a lot of \"starter\" code here that will not work out of the box.  You'll need to add your own code to fill in some gaps that are missing.  Ask Prof. Fisher if you get stuck.\n",
    "\n",
    "## Part 2a:\n",
    "\n",
    "This is almost exactly the same as what we did for Homework Set 2 Problem 1a, so feel free to copy that code over to the next cell!  Make the following edits:\n",
    "\n",
    "  1. Increase the number of training data points from 10 to 100, but keep the sampling of the x range random.\n",
    "  2. Change the functional form for the targets (```ytrain```) to be $y=3x-2x^2+2x^3-2x^4+0.3\\sin(x\\times 15)$.  For the training data, add some Gaussian noise (ie, add $+ \\rm{Gaus}(0,0.3)$ to ytrain).\n",
    "  3. We still want to order our training data to improve plotting, but we also need a copy of the randomized data.  It would be convenient if you made that copy here.\n",
    "  \n",
    "Keep the testing data as a convenient way to plot the true function.  You shouldn't add the noise to the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-petite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and visualize the data\n",
    "\n",
    "# Make some training data\n",
    "nPts = 100\n",
    "np.random.seed(119)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-october",
   "metadata": {},
   "source": [
    "## Part 2b:\n",
    "\n",
    "Create a design/feature matrix for your samples.  Do this in the same way that you did for Problem 1b.  Use a 7th order polynomial (nDegr=7).  Again, I strongly encourage you to try out the module ```PolynomialFeatures``` from ```scikit-learn```.  Don't forget your randomized training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-antibody",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "improved-taylor",
   "metadata": {},
   "source": [
    "## Part 2c:\n",
    "\n",
    "Use the Numpy ```linalg``` module to calculate the vector of weights that best fit your data.  Make a plot of your training data and your best fit line.  Also plot your testing data to illustrate how well the linear regression fit reproduces your underlying true function.\n",
    "\n",
    "A convenient way to generate your line would be like this:  ```line = np.dot(DM,betas)``` wherein ```betas``` is the weights vector from your ```linalg``` solution.  Just like Problem #1c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-works",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "smooth-economics",
   "metadata": {},
   "source": [
    "## Part 2d:\n",
    "\n",
    "Here we will explore two strategies for generating mini-batches for performing cross-validation training.  The first strategy will use k-fold cross validation.  This is performed by breaking your data sample into k orthogonal subsamples and training on each subsample independently.  The second strategy will be to generate bootstrap data samples, wherein the training data are randomly sampled (with replacement).  In each case, you will use a range of mini-batch sizes, ranging from small (10%) to big (50%).  The way you will use these mini-batches is to fit your polynomial function to each mini-batch and average over all mini-batches to find an aggregate \"best fit\".  For example, if you have a mini-batch size of 10%, you will have 10 mini-batches and, thus, 10 regression results to average.\n",
    "\n",
    "To avoid spending your time in Python dev mode, most of the code has been provided.  Your job will be to create functions that properly generate the mini-baches for training.  Do not use ```scikit-learn``` for this, as the goal is to generate your own intuition for how the algorithm should work.  However, the use of ```Numpy``` is stronly encouraged.  You can either set up your code to perform both k-fold cross validation and bootstrap sampling at once, or allow toggling between the two. \n",
    "\n",
    "Some things to think about:\n",
    "\n",
    "  1. When you make your mini-batches, you need to provide subsamples for both the design matrix and for the targets.\n",
    "  2. Your mini-batches must maintain the one-to-one relationship of design matrix rows to targets.  This will be more challenging with the bootstrap random sampling.  I suggest exploring the ```Numpy::random.choice()``` method.\n",
    "\n",
    "The code provided uses L2 regularization and has hyperparameters set to allow good convergence of the range of batch fits being explored.  Feel free to change them to see how things vary, but when you turn in your homework please keep the original values.  For example, you might be interested to see what the effect of removing L2 regularization might be.\n",
    "\n",
    "You will need to make plots of your results in the next part of the problem.  So you need to add two arrays to keep track of your results:\n",
    "\n",
    "  1. An array to average the fit parameters from each mini-batch.  For example, if you choose mini-batch size to be 10%, you will have 10 mini-batches to average over.  This array should be extended to hold all of your mini-batch size averages.  The size should be ```(batchRange.size,nDegr+1)```.\n",
    "  2. An array to hold the weights for fits to each of the mini-batches in your smallest mini-batch size.  This will be to illustrate the power of averaging over mini-batches.  The size should be ```(nBatches,nDegr+1)```.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-courage",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear regression\n",
    "\n",
    "# some hyperparameters\n",
    "step_size = 0.2  # Note that if the learning rate is too big, you may not converge!\n",
    "reg = 0.003 # regularization strength, let's stick to L2 reg this time\n",
    "\n",
    "step = 0.1  # number of steps for the batch size to be tested over\n",
    "bMin = 0.1\n",
    "bMax = 0.51\n",
    "\n",
    "doRandomBatch = 0\n",
    "\n",
    "# this array will hold the values of the regularization strength that we'll test\n",
    "batchRange = np.arange(bMin,bMax+0.001,step)\n",
    "\n",
    "\n",
    "# gradient descent loop\n",
    "Niter = 2500\n",
    "for bidx, batchFrac in enumerate(batchRange):\n",
    "\n",
    "    #calculate the number of batches we can get using this fraction\n",
    "    batchIterations = np.int(0.01+1.0/batchFrac)\n",
    "    avgWeights = np.zeros(betas.size)\n",
    "    \n",
    "    print(\"Batch Size Fraction:\",batchFrac)\n",
    "    for aidx in range(batchIterations):\n",
    "        \n",
    "        # build a mini-batch!\n",
    "        if doRandomBatch: # bootstrap data samples\n",
    "            iDM, iy = ?\n",
    "        else: # k-fold cross validation data samples\n",
    "            iDM, iy = ?\n",
    "        \n",
    "        #Start from our analytical fit to speed convergence\n",
    "        W = betas*0.99\n",
    "        for i in range(Niter):\n",
    "  \n",
    "            # evaluate function values\n",
    "            fhat = np.dot(iDM,W)\n",
    "\n",
    "            # compute mean squared loss\n",
    "            data_loss = np.sum((iy-fhat)**2)\n",
    "            data_loss /= iy.size\n",
    "\n",
    "            # Regularize!\n",
    "            # L2 regularization\n",
    "            reg_loss = 0.5*reg*np.sum(W*W)\n",
    "\n",
    "            # Total loss is the sum\n",
    "            loss = reg_loss + data_loss\n",
    "\n",
    "            # Report progress\n",
    "            if i % 1000 == 0:\n",
    "                print(\"Batch %.3f, iteration %d: loss %f\" % (batchFrac,i, loss))\n",
    "        \n",
    "            #compute the loss gradients\n",
    "            dW = -2*np.dot(iDM.T,iy-fhat)\n",
    "            dW /= iy.size\n",
    "        \n",
    "            dW += reg*W # regularization gradient for L2\n",
    "        \n",
    "            # update the weights\n",
    "            W -= step_size*dW\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-faculty",
   "metadata": {},
   "source": [
    "## Part 2e:\n",
    "\n",
    "Now that you have your mini-batch results, create two figures:\n",
    "\n",
    "  1. A plot of your training and test samples overlaid with all N of the fits from your smallest mini-batch fraction.  For example, if your smallest mini-batch fraction is 10%, you'll have 10 lines overlaid.\n",
    "  2. A plot of your training and test samples overlaid with all M of your mini-batch fit averages.\n",
    "  \n",
    "You should be able to make these figures for both k-fold cross validation and for random bootstrap sampling.  Once you've inspected all four figures, please answer the following questions:\n",
    "\n",
    "  1. Which ensemble method give you the smallest variance between mini-batch sizes?  Why do you think this is?\n",
    "  2. Which ensemble method reproduces the true underlying function the best?  Is that a good thing or not in this case?\n",
    "  3. What are the apparent advantages of large or small mini-batch sizes?\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-corps",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
